---
title: "ST558 Project 2"
author: "Sergio Mora & Ashley Ko"
output: html_document
params:
  category: "lifestyle"
---


```{r setup, include=FALSE, message=FALSE}
set.seed(123)

#Required Packages
library(knitr)
library(rmarkdown)
library(caret)
library(tidyverse)
library(corrplot)
```  

# Introduction

# Data
## Reading in data
We will read in our csv dataset. As instructed we will also split our data by `data_channel_is_*`.
```{r}
online_new_popularity_data <- read.csv("./OnlineNewsPopularity/OnlineNewsPopularity.csv")
```

## Subsetting the data

We will subset the data based on the category listed in our YAML. In this case we will be using ``r paste0("data_channel_is_",params$category)``.
```{r}
#Subsetting our data based on the category parameter
subset_data <- online_new_popularity_data %>% filter(!!as.name(paste0("data_channel_is_",params$category)) == 1) %>% select(-starts_with("data_channel_is"))
```

Next we will check for potential problematic values such as NA or infinity. These could result in errors with later analysis. Should a problem arise later on, this allows for a diagnoistic to rule out potential problematic values.
```{r}
#Checking data for NA  or infinite values
apply(subset_data, 2, function(x) any(is.na(x) | is.infinite(x)))
```


```{r}
#Setting up a simple 70/30 split for our already subset data
sample_size <- floor(0.7 * nrow(subset_data)) 
train_ind <- sample(seq_len(nrow(subset_data)), size = sample_size)

# This will be needed later on when we start modeling
training_data <- subset_data[train_ind,]
test_data <- subset_data[-train_ind,]
```

# Summarizations

## Numeric Summary

### Six Number Summary
First let's perform a simple numeric summary variables to calculate a six number summary for each variable from the training data set. This summary includes minimum, 1st quartile, median, mean, 3rd quartile, and maximum values. This provides a senses of scale and range for variable values.
```{r}
summary(training_data)
```

### Standard Deviation
The previous section does not generate standard deviation for the variable values. Standard deviation is necessary for determining the variance of the response and predictors. It is a good diagnostic to spot potential issues that violate assumptions necessary for models and analysis. For the purpose of this numeric summary, the variable `url` was subset out of `training_data` as its values are non-numeric.
```{r}
train_no_url <- training_data %>% select(timedelta:shares)
options(scipen = 999)
train_SDs <- sapply(train_no_url, sd)
train_SDs
options(scipen = 0)
```

### IQR
Although the 1st and 3rd quartiles are identified in the six number summary, it is helpful quantify the range between these two values, IQR. IQR is also needed for subsequent plotting. Binary response variables such as `weekday_is_*` and `is_weekend` have values of 0 given the nature of those predictors. Additionally, it is necessary to note that the `train_no_url` data was used to generate these results. 
```{r}
IQRs <- as_tibble(lapply(train_no_url, IQR))
IQRs
```

### Correlations
Prior to preforming any model fitting or statistical analysis it is essential to understand the potential correlation among predictors and between the response and predictors. Correlation helps identify potential collinearity and thus, allows for better candidate model selection. It is worth noting any absolute correlation values > 0.5. However, this threshold has been left to discretion of the individual.
```{r}
variables <- as_tibble(attributes(train_no_url)$names) %>%
  rename(variable = "value")

corr <- cor(train_no_url)
round(corr, 3)

Correlations <-as_tibble(corr)
Correlations <- (rownames = attributes(train_no_url)$names)

corr_mat <- bind_cols(variables, Correlations)
correlation_matrix <- column_to_rownames(corr_mat, var = "variable")
correlation_matrix
```
### Contigency tables

## Graphical summaries



Our idea is in part that what makes a link shareable is how easy it is for the content to be consumed. People want to be spoon fed information. We will test this out via proxy's. We will measure shares against average key words(kw_avg_avg), average length of words (average_token_length), average number of words in the content (n_tokens_content), and number of words in the title (n_tokens_title). The idea here is to measure both the quantity of words as well as the complexity of the content. i.e. an article with 500 "easy" words could be shared more than an article with 100 "difficult" words.



Now let's clean our data. If we have any outliers we will remove them first to get an idea of what the bulk of shares come from. We will follow what the boxplot tells us when choosing what to remove.
```{r}
boxplot(subset_data$shares,horizontal = TRUE, range = 2, main = "Boxplot of shares with outliers")

boxplot(subset_data$shares,horizontal = TRUE, range = 2, outline = FALSE,main = "Boxplot of shares without outliers")
# We can have some pretty extreme values
IQR <- quantile(subset_data$shares)[4] - quantile(subset_data$shares)[2]
upper_limit <- quantile(subset_data$shares)[4] + (1.5 * IQR)
lower_limit <- quantile(subset_data$shares)[2] - (1.5 * IQR)
subset_data_wo_outliers <- subset_data %>% filter(shares <= upper_limit & shares >= lower_limit)
```

After we remove any potential outliers to our data our we can compare shares our key metrics.
```{r shares Vs Keywords average, warning=FALSE}
correlation1 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$kw_avg_avg)

plot1 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = kw_avg_avg)) + 
  geom_point() +
  geom_smooth() +
  labs(title = "Number of shares vs. Average number of key words", y= "# of shares", x = "Average # of key words") +
  geom_text(color = "red",x=15000,y=5000,label = paste0("Correlation = ",round(correlation1,3)))

plot1
```

We can measure the trend of shares as a function of Average number of key words. If we see a possitive trend we can say that the more key words in the articles the more likely it is to be shared, the opposite can also be said. We measure the correlation to get a more precise gauge in case the graph is not clear enough.


```{r shares Vs Average length of words in content, warning=FALSE}
correlation2 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$average_token_length)

plot2 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = average_token_length)) +
geom_density_2d() + 
  labs(title = "number of shares vs. Average length of words in content", y= "# of shares", x = "Average length of words in content") +
  geom_text(color = "red",x=5,y=3500,label = paste0("Correlation = ",round(correlation2,3)))

plot2
```

With a density plot as a function of average length of words in content we see where most of our shares come from. We can utilize this to help explain our model down below.


```{r, warning=FALSE}
correlation3 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_content)

plot3 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_content)) +
geom_rug() +
  labs(title = "number of shares vs. number of words in content", y= "# of shares", x = "# of words in content") +
  geom_text(color = "red",x=4000,y=4000,label = paste0("Correlation = ",round(correlation3,3)))

plot3
```

Using a rug graph we can measure the relationship between number of words in content and the number of shares. The intersection between where both rugs are highly concentrated is where how we can measure correlation. If both rugs are concentrated near zero than we see that the less words the more shareable the articles are or vice versa.

```{r warning=FALSE}
correlation4 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_title)

plot4 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_title)) +
geom_col() +
  labs(title = "number of shares vs. number of words in title", y= "# of shares", x = "# of words in title") +
  geom_text(color = "red",x=15,y=600000,label = paste0("Correlation = ",round(correlation4,3)))

plot4
```





# Modeling

# Comparison