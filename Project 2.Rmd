---
title: "ST558 Project 2"
author: "Sergio Mora & Ashley Ko"
output: html_document
params:
  category: "lifestyle"
---



```{r setup, include=FALSE, message=FALSE}
set.seed(123)

#Required Packages
library(knitr)
library(rmarkdown)
library(caret)
library(tidyverse)
library(dplyr)
```  

# Introduction

# Data
## Reading in data
We will read in our csv dataset. As instructed we will also split our data by `data_channel_is_*`.
```{r}
online_new_popularity_data <- read.csv("./OnlineNewsPopularity/OnlineNewsPopularity.csv")
```

## Subsetting the data
We will subset the data based on the category listed in our YAML. In this case we will be using ``r paste0("data_channel_is_",params$category)``.
```{r}
#Subsetting our data based on the category parameter
subset_data <- online_new_popularity_data %>% filter(!!as.name(paste0("data_channel_is_",params$category)) == 1) %>% select(-starts_with("data_channel_is"))

#Setting up a simple 70/30 split for our already subset data
sample_size <- floor(0.7 * nrow(subset_data)) 
train_ind <- sample(seq_len(nrow(subset_data)), size = sample_size)

# This will be needed later on when we start modeling
training_data <- subset_data[train_ind,]
test_data <- subset_data[-train_ind,]

```

# Summarizations

Our idea is in part that what makes a link shareable is how easy it is for the content to be consumed. People want to be spoon fed information. We will test this out via proxy's. We will measure shares against average key works(kw_avg_avg),average length of words (average_token_length), average number of words in the content (n_tokens_content) ,and number of words in the title (n_tokens_title). The idea here is to measure both the quantity of words as well as the complexity of the content. i.e. an article with 500 "easy" words could be shared more than an article with 100 "difficult" words.

## EDA's

First let's start off by cleaning our data. If we have any outliers we will remove them first to get an idea of what the bulk of shares come from. We will follow what the boxplot tells us when choosing what to remove.
```{r}
boxplot(subset_data$shares,horizontal = TRUE, range = 2, main = "Boxplot of shares with outliers")

boxplot(subset_data$shares,horizontal = TRUE, range = 2, outline = FALSE,main = "Boxplot of shares without outliers")
# We can have some pretty extreme values
IQR <- quantile(subset_data$shares)[4] - quantile(subset_data$shares)[2]
upper_limit <- quantile(subset_data$shares)[4] + (1.5 * IQR)
lower_limit <- quantile(subset_data$shares)[2] - (1.5 * IQR)
subset_data_wo_outliers <- subset_data %>% filter(shares <= upper_limit & shares >= lower_limit)
```

After we remove any potential outliers to our data our we can compare shares our key metrics.
```{r shares Vs Keywords average, warning=FALSE}
correlation1 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$kw_avg_avg)

plot1 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = kw_avg_avg)) + 
  geom_point() +
  geom_smooth() +
  labs(title = "number of shares vs. Average number of key words", y= "# of shares", x = "Average # of key words") +
  geom_text(color = "red",x=15000,y=5000,label = paste0("Correlation = ",round(correlation1,3)))

plot1
```

We can measure the trend of shares as a function of Average number of key words. If we see a possitive trend we can say that the more key words in the articles the more likely it is to be shared, the opposite can also be said. We measure the correlation to get a more precise gauge incase the graph is not clear enough.


```{r shares Vs Average length of words in content, warning=FALSE}
correlation2 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$average_token_length)

plot2 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = average_token_length)) +
geom_density_2d() + 
  labs(title = "number of shares vs. Average length of words in content", y= "# of shares", x = "Average length of words in content") +
  geom_text(color = "red",x=5,y=3500,label = paste0("Correlation = ",round(correlation2,3)))

plot2
```

With a density plot as a function of average length of words in content we see where most of our shares come from. We can utilize this to help explain our model down below.


```{r, warning=FALSE}
correlation3 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_content)

plot3 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_content)) +
geom_rug() +
  labs(title = "number of shares vs. number of words in content", y= "# of shares", x = "# of words in content") +
  geom_text(color = "red",x=4000,y=4000,label = paste0("Correlation = ",round(correlation3,3)))

plot3
```

Using a rug graph we can measure the relationship between number of words in content and the number of shares. The intersection between where both rugs are highly concentrated is where how we can measure correlation. If both rugs are concentrated near zero than we see that the less words the more shareable the articles are or vice versa.

```{r warning=FALSE}
correlation4 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_title)

plot4 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_title)) +
geom_col() +
  labs(title = "number of shares vs. number of words in title", y= "# of shares", x = "# of words in title") +
  geom_text(color = "red",x=15,y=600000,label = paste0("Correlation = ",round(correlation4,3)))

plot4
```



## Contigency tables

# Modeling

# Comparison