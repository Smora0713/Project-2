plot4
# Note need to probably name graphs still etc...work in progress
corrplot(cor(training_data), tl.col = "black")
corNoOut<- function(x) {
var1 <- get(x, subset_data_wo_outliers)
title <-paste0("Scatterplot of shares vs ", x)
xlabel <- x
g <- ggplot(subset_data_wo_outliers, aes(x = var1, y = shares)) + geom_point() +
geom_smooth() + geom_smooth(method = lm, col = "Red") +
labs(title = title, y= "# of shares", x = xlabel)
}
sentiment_preds <- list("global_subjectivity", "global_sentiment_polarity",
"global_rate_negative_words", "avg_negative_polarity",
"abs_title_subjectivity", "abs_title_sentiment_polarity")
lapply(sentiment_preds, corNoOut)
# Removing scientific notation for readability
options(scipen = 999)
# Coverts is_weekend into a two level factor
weekend_factor <- subset_data_wo_outliers %>%
mutate(weekend = as.factor(is_weekend)) %>% select(weekend, shares)
# Base plot
g <- ggplot(weekend_factor, aes(x=shares)) +  xlab("Number of Shares") +
ylab("Density")
# Filled, stacked histogram with density of shares by weekend level
g + geom_histogram(aes(y = ..density.., fill = weekend)) +
geom_density(adjust = 0.25, alpha = 0.5, aes(fill = weekend), position = "stack") +
labs(title = "Density of Shares: Weekday vs. Weekend") +
scale_fill_discrete(name = "Weekday or Weekend?", labels = c("Weekday", "Weekend"))
# Turning on scientific notation
options(scipen = 0)
# First linear model
lmfit1 <- lm(shares ~kw_avg_avg*average_token_length*n_tokens_content*n_tokens_title, data = training_data)
summary(lmfit1)
# Generalates second linear model candidate
lmfit2 <- lm(shares ~  n_tokens_content*num_imgs + n_tokens_content*num_videos +
n_tokens_content:average_token_length + n_tokens_content*kw_avg_avg +
n_tokens_content*global_sentiment_polarity +
n_tokens_content*global_rate_negative_words + num_imgs*kw_avg_avg +
num_imgs*abs_title_sentiment_polarity + num_videos*average_token_length +
num_videos*global_subjectivity + num_videos*global_sentiment_polarity +
num_videos*global_rate_negative_words + num_videos*abs_title_sentiment_polarity,
data = training_data)
summary(lmfit2)
fitTree <- tree(shares ~kw_avg_avg + average_token_length + n_tokens_content + n_tokens_title, data = training_data)
plot(fitTree)
text(fitTree)
#Random Forest
#Train control options for ensemble models
trCtrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
rfFit <- train(shares ~kw_avg_avg + average_token_length + n_tokens_content + n_tokens_title, data = training_data, method = "rf",trControl=trCtrl, preProcess = c("center", "scale"),tuneGrid = data.frame(mtry = 1:4))
plot(rfFit)
#Tune Parameters for cross-validation
tuneParam <-expand.grid(n.trees = c(10, 25, 50, 100, 150, 200),
interaction.depth = 1:4, shrinkage = 0.1,
n.minobsinnode = 10)
#Cross-validation, tune parameter selection, and training of boosted tree models
boostTree <-train(shares ~ ., data = training_data, method = "gbm",
trControl = trCtrl, preProcess = c("center","scale"),
tuneGrid = tuneParam)
#Plot of RMSE by Max Tree Depth
plot(boostTree)
#Results from model training
boostTree$results
#Best tuning parameters
boostTree$bestTune
#Uses best tuned training boosted tree model to predict test data
boostTreePred <-predict(boostTree, newdata = test_data)
#Reports best boosted tree model and corresponding RMSE
boost <-boostTree$results[1,]
boost
lmfit_1 = c(MAE(test_data$shares,predict(lmfit1)),RMSE(test_data$shares,predict(lmfit1)))
lmfit_2 = c(MAE(test_data$shares,predict(lmfit2)),RMSE(test_data$shares,predict(lmfit2)))
rffit_c = c(MAE(test_data$shares,predict(rfFit)),RMSE(test_data$shares,predict(rfFit)))
boostTree_c = c(MAE(test_data$shares,predict(boostTree)),RMSE(test_data$shares,predict(boostTree)))
MAE_RMSE_SUMM <- rbind.data.frame("Linear Model 1" = lmfit_1, "Linear Model 2" = lmfit_2,"Random Forrest" = rffit_c, "Boosted Tree" = boostTree_c)
colnames(MAE_RMSE_SUMM) <- c("MAE","RMSE")
rownames(MAE_RMSE_SUMM) <- c("Linear Model 1", "Linear Model 2", "Random Forrest", "Boosted Tree")
kable(MAE_RMSE_SUMM, caption = "Comparing models via MAE and RMSE")
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(parallel)
library(doParallel)
library(rmarkdown)
# Creates a list of the six data_channel_is_* options
#category <- c("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
category <- c("lifestyle","entertainment","bus")
# Creates output filenames
output_file <- paste0(category, ".html")
# Creates a list for each data channel with just the category parameter
params = lapply(category, FUN = function(x){list(category = x)})
# Stores list of file output file names and parameters as a data frame
reports <- tibble(output_file, params)
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project 2.Rmd",
output_file = x[[1]],
params = x[[2]])})
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(parallel)
library(doParallel)
library(rmarkdown)
# Creates a list of the six data_channel_is_* options
category <- c("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
# Creates output filenames
output_file <- paste0(category, ".html")
# Creates a list for each data channel with just the category parameter
params = lapply(category, FUN = function(x){list(category = x)})
# Stores list of file output file names and parameters as a data frame
reports <- tibble(output_file, params)
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project 2.Rmd",
output_file = x[[1]],
params = x[[2]])})
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project 2.Rmd",
output_file = x[[1]],
params = x[[2]])})
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project 2.Rmd",
output_file = x[[1]],
params = x[[2]])})
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(parallel)
library(doParallel)
library(rmarkdown)
# Creates a list of the six data_channel_is_* options
category <- c("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
# Creates output filenames
output_file <- paste0(category, ".html")
# Creates a list for each data channel with just the category parameter
params = lapply(category, FUN = function(x){list(category = x)})
# Stores list of file output file names and parameters as a data frame
reports <- tibble(output_file, params)
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project 2.Rmd",
output_file = x[[1]],
params = x[[2]])})
library(rmarkdown)
# Creates a list of the six data_channel_is_* options
category <- c("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
# Creates output filenames
output_file <- paste0(category, ".html")
# Creates a list for each data channel with just the category parameter
params = lapply(category, FUN = function(x){list(category = x)})
# Stores list of file output file names and parameters as a data frame
reports <- tibble(output_file, params)
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project 2.Rmd",
output_file = x[[1]],
params = x[[2]])})
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(parallel)
library(doParallel)
#Reads in csv file `OnlineNewsPopularity/OnlineNewsPopularity.csv` to produce a data frame
online_new_popularity_data <- read.csv("./OnlineNewsPopularity/OnlineNewsPopularity.csv")
# Subsetting our data based on the category parameter, dropping non-predictors,
# and subsetting for desired predictors and response variables
subset_data <- online_new_popularity_data %>%
filter(!!as.name(paste0("data_channel_is_",params$category)) == 1) %>%
select(n_tokens_title, n_tokens_content, num_imgs:average_token_length,
kw_avg_avg, is_weekend, global_subjectivity, global_sentiment_polarity,
global_rate_negative_words, avg_negative_polarity, abs_title_subjectivity,
abs_title_sentiment_polarity, shares)
# Checking data for NA  or infinite values
na_or_infinite <- as.data.frame(apply(subset_data, 2, function(x) any(is.na(x) | is.infinite(x))))
colnames(na_or_infinite) <- c("NA or Infinite values")
na_or_infinite %>% kable()
# Setting up a simple 70/30 split for our already subset data
sample_size <- floor(0.7 * nrow(subset_data))
train_ind <- sample(seq_len(nrow(subset_data)), size = sample_size)
# This will be needed later on when we start modeling
training_data <- subset_data[train_ind,]
test_data <- subset_data[-train_ind,]
#Generates a six number summary from training_data
summary <- summary(training_data)
summary
# Removes scientific notation for readability
options(scipen = 999)
# Generates and rounds (for simplicity) standard deviation
train_SDs <- as_tibble(lapply(training_data[, 1:14], sd))
round(train_SDs, digits = 5)
# Turns scientific notation on
options(scipen = 0)
# Uses lapply to generate IQR for all variables in training_data
IQR<- as_tibble(lapply(training_data[, 1:14], IQR))
IQR
# Creating a data frame of a single column of variable names
variables <- as_tibble(attributes(training_data)$names) %>%
rename(variable = "value")
# Generates correlations for all variables in training_data
corr <- cor(training_data)
correlations <- as_tibble(round(corr, 3))
# Binds the variable names to the correlation data frame
corr_mat <- bind_cols(variables, correlations)
correlation_matrix <- column_to_rownames(corr_mat, var = "variable")
correlation_matrix
# Boxplot from training_data
boxplot(training_data$shares,horizontal = TRUE, range = 2, main = "Boxplot of shares with outliers")
boxplot(training_data$shares,horizontal = TRUE, range = 2, outline = FALSE,main = "Boxplot of shares without outliers")
# Creates a subset of training data without outliers for #plotting purposes
IQR <- quantile(training_data$shares)[4] - quantile(subset_data$shares)[2]
upper_limit <- quantile(training_data$shares)[4] + (1.5 * IQR)
lower_limit <- quantile(training_data$shares)[2] - (1.5 * IQR)
subset_data_wo_outliers <- training_data %>% filter(shares <= upper_limit & shares >= lower_limit)
correlation1 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$kw_avg_avg)
plot1 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = kw_avg_avg)) +
geom_point() +
geom_smooth() +
labs(title = "Number of shares vs. Average number of key words", y= "# of shares", x = "Average # of key words") +
geom_text(color = "red",x=15000,y=5000,label = paste0("Correlation = ",round(correlation1,3)))
plot1
correlation2 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$average_token_length)
plot2 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = average_token_length)) +
geom_density_2d() +
labs(title = "number of shares vs. Average length of words in content", y= "# of shares", x = "Average length of words in content") +
geom_text(color = "red",x=5,y=3500,label = paste0("Correlation = ",round(correlation2,3)))
plot2
correlation3 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_content)
plot3 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_content)) +
geom_rug() +
labs(title = "number of shares vs. number of words in content", y= "# of shares", x = "# of words in content") +
geom_text(color = "red",x=4000,y=4000,label = paste0("Correlation = ",round(correlation3,3)))
plot3
correlation4 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_title)
plot4 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_title)) +
geom_col() +
labs(title = "number of shares vs. number of words in title", y= "# of shares", x = "# of words in title") +
geom_text(color = "red",x=15,y=600000,label = paste0("Correlation = ",round(correlation4,3)))
plot4
# Note need to probably name graphs still etc...work in progress
corrplot(cor(training_data), tl.col = "black")
corNoOut<- function(x) {
var1 <- get(x, subset_data_wo_outliers)
title <-paste0("Scatterplot of shares vs ", x)
xlabel <- x
g <- ggplot(subset_data_wo_outliers, aes(x = var1, y = shares)) + geom_point() +
geom_smooth() + geom_smooth(method = lm, col = "Red") +
labs(title = title, y= "# of shares", x = xlabel)
}
sentiment_preds <- list("global_subjectivity", "global_sentiment_polarity",
"global_rate_negative_words", "avg_negative_polarity",
"abs_title_subjectivity", "abs_title_sentiment_polarity")
lapply(sentiment_preds, corNoOut)
# Removing scientific notation for readability
options(scipen = 999)
# Coverts is_weekend into a two level factor
weekend_factor <- subset_data_wo_outliers %>%
mutate(weekend = as.factor(is_weekend)) %>% select(weekend, shares)
# Base plot
g <- ggplot(weekend_factor, aes(x=shares)) +  xlab("Number of Shares") +
ylab("Density")
# Filled, stacked histogram with density of shares by weekend level
g + geom_histogram(aes(y = ..density.., fill = weekend)) +
geom_density(adjust = 0.25, alpha = 0.5, aes(fill = weekend), position = "stack") +
labs(title = "Density of Shares: Weekday vs. Weekend") +
scale_fill_discrete(name = "Weekday or Weekend?", labels = c("Weekday", "Weekend"))
# Turning on scientific notation
options(scipen = 0)
# First linear model
lmfit1 <- lm(shares ~kw_avg_avg*average_token_length*n_tokens_content*n_tokens_title, data = training_data)
summary(lmfit1)
# Generalates second linear model candidate
lmfit2 <- lm(shares ~  n_tokens_content*num_imgs + n_tokens_content*num_videos +
n_tokens_content:average_token_length + n_tokens_content*kw_avg_avg +
n_tokens_content*global_sentiment_polarity +
n_tokens_content*global_rate_negative_words + num_imgs*kw_avg_avg +
num_imgs*abs_title_sentiment_polarity + num_videos*average_token_length +
num_videos*global_subjectivity + num_videos*global_sentiment_polarity +
num_videos*global_rate_negative_words + num_videos*abs_title_sentiment_polarity,
data = training_data)
summary(lmfit2)
fitTree <- tree(shares ~kw_avg_avg + average_token_length + n_tokens_content + n_tokens_title, data = training_data)
plot(fitTree)
text(fitTree)
#Random Forest
#Train control options for ensemble models
trCtrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
rfFit <- train(shares ~kw_avg_avg + average_token_length + n_tokens_content + n_tokens_title, data = training_data, method = "rf",trControl=trCtrl, preProcess = c("center", "scale"),tuneGrid = data.frame(mtry = 1:4))
plot(rfFit)
#Tune Parameters for cross-validation
tuneParam <-expand.grid(n.trees = c(10, 25, 50, 100, 150, 200),
interaction.depth = 1:4, shrinkage = 0.1,
n.minobsinnode = 10)
#Cross-validation, tune parameter selection, and training of boosted tree models
boostTree <-train(shares ~ ., data = training_data, method = "gbm",
trControl = trCtrl, preProcess = c("center","scale"),
tuneGrid = tuneParam)
#Plot of RMSE by Max Tree Depth
plot(boostTree)
#Results from model training
boostTree$results
#Best tuning parameters
boostTree$bestTune
#Uses best tuned training boosted tree model to predict test data
boostTreePred <-predict(boostTree, newdata = test_data)
#Reports best boosted tree model and corresponding RMSE
boost <-boostTree$results[1,]
boost
lmfit_1 = c(MAE(test_data$shares,predict(lmfit1)),RMSE(test_data$shares,predict(lmfit1)))
lmfit_2 = c(MAE(test_data$shares,predict(lmfit2)),RMSE(test_data$shares,predict(lmfit2)))
rffit_c = c(MAE(test_data$shares,predict(rfFit)),RMSE(test_data$shares,predict(rfFit)))
boostTree_c = c(MAE(test_data$shares,predict(boostTree)),RMSE(test_data$shares,predict(boostTree)))
MAE_RMSE_SUMM <- rbind.data.frame("Linear Model 1" = lmfit_1, "Linear Model 2" = lmfit_2,"Random Forrest" = rffit_c, "Boosted Tree" = boostTree_c)
colnames(MAE_RMSE_SUMM) <- c("MAE","RMSE")
rownames(MAE_RMSE_SUMM) <- c("Linear Model 1", "Linear Model 2", "Random Forrest", "Boosted Tree")
kable(MAE_RMSE_SUMM, caption = "Comparing models via MAE and RMSE")
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(parallel)
library(doParallel)
lmfit_1 = c(MAE(test_data$shares,predict(lmfit1)),RMSE(test_data$shares,predict(lmfit1)))
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(parallel)
library(doParallel)
#Reads in csv file `OnlineNewsPopularity/OnlineNewsPopularity.csv` to produce a data frame
online_new_popularity_data <- read.csv("./OnlineNewsPopularity/OnlineNewsPopularity.csv")
# Subsetting our data based on the category parameter, dropping non-predictors,
# and subsetting for desired predictors and response variables
subset_data <- online_new_popularity_data %>%
filter(!!as.name(paste0("data_channel_is_",params$category)) == 1) %>%
select(n_tokens_title, n_tokens_content, num_imgs:average_token_length,
kw_avg_avg, is_weekend, global_subjectivity, global_sentiment_polarity,
global_rate_negative_words, avg_negative_polarity, abs_title_subjectivity,
abs_title_sentiment_polarity, shares)
# Checking data for NA  or infinite values
na_or_infinite <- as.data.frame(apply(subset_data, 2, function(x) any(is.na(x) | is.infinite(x))))
colnames(na_or_infinite) <- c("NA or Infinite values")
na_or_infinite %>% kable()
# Setting up a simple 70/30 split for our already subset data
sample_size <- floor(0.7 * nrow(subset_data))
train_ind <- sample(seq_len(nrow(subset_data)), size = sample_size)
# This will be needed later on when we start modeling
training_data <- subset_data[train_ind,]
test_data <- subset_data[-train_ind,]
#Generates a six number summary from training_data
summary <- summary(training_data)
summary
# Removes scientific notation for readability
options(scipen = 999)
# Generates and rounds (for simplicity) standard deviation
train_SDs <- as_tibble(lapply(training_data[, 1:14], sd))
round(train_SDs, digits = 5)
# Turns scientific notation on
options(scipen = 0)
# Uses lapply to generate IQR for all variables in training_data
IQR<- as_tibble(lapply(training_data[, 1:14], IQR))
IQR
# Creating a data frame of a single column of variable names
variables <- as_tibble(attributes(training_data)$names) %>%
rename(variable = "value")
# Generates correlations for all variables in training_data
corr <- cor(training_data)
correlations <- as_tibble(round(corr, 3))
# Binds the variable names to the correlation data frame
corr_mat <- bind_cols(variables, correlations)
correlation_matrix <- column_to_rownames(corr_mat, var = "variable")
correlation_matrix
# Boxplot from training_data
boxplot(training_data$shares,horizontal = TRUE, range = 2, main = "Boxplot of shares with outliers")
boxplot(training_data$shares,horizontal = TRUE, range = 2, outline = FALSE,main = "Boxplot of shares without outliers")
# Creates a subset of training data without outliers for #plotting purposes
IQR <- quantile(training_data$shares)[4] - quantile(subset_data$shares)[2]
upper_limit <- quantile(training_data$shares)[4] + (1.5 * IQR)
lower_limit <- quantile(training_data$shares)[2] - (1.5 * IQR)
subset_data_wo_outliers <- training_data %>% filter(shares <= upper_limit & shares >= lower_limit)
correlation1 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$kw_avg_avg)
plot1 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = kw_avg_avg)) +
geom_point() +
geom_smooth() +
labs(title = "Number of shares vs. Average number of key words", y= "# of shares", x = "Average # of key words") +
geom_text(color = "red",x=15000,y=5000,label = paste0("Correlation = ",round(correlation1,3)))
plot1
correlation2 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$average_token_length)
plot2 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = average_token_length)) +
geom_density_2d() +
labs(title = "number of shares vs. Average length of words in content", y= "# of shares", x = "Average length of words in content") +
geom_text(color = "red",x=5,y=3500,label = paste0("Correlation = ",round(correlation2,3)))
plot2
correlation3 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_content)
plot3 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_content)) +
geom_rug() +
labs(title = "number of shares vs. number of words in content", y= "# of shares", x = "# of words in content") +
geom_text(color = "red",x=4000,y=4000,label = paste0("Correlation = ",round(correlation3,3)))
plot3
correlation4 <- cor(subset_data_wo_outliers$shares,subset_data_wo_outliers$n_tokens_title)
plot4 <- ggplot(subset_data_wo_outliers, aes(y= shares,x = n_tokens_title)) +
geom_col() +
labs(title = "number of shares vs. number of words in title", y= "# of shares", x = "# of words in title") +
geom_text(color = "red",x=15,y=600000,label = paste0("Correlation = ",round(correlation4,3)))
plot4
# Note need to probably name graphs still etc...work in progress
corrplot(cor(training_data), tl.col = "black")
corNoOut<- function(x) {
var1 <- get(x, subset_data_wo_outliers)
title <-paste0("Scatterplot of shares vs ", x)
xlabel <- x
g <- ggplot(subset_data_wo_outliers, aes(x = var1, y = shares)) + geom_point() +
geom_smooth() + geom_smooth(method = lm, col = "Red") +
labs(title = title, y= "# of shares", x = xlabel)
}
sentiment_preds <- list("global_subjectivity", "global_sentiment_polarity",
"global_rate_negative_words", "avg_negative_polarity",
"abs_title_subjectivity", "abs_title_sentiment_polarity")
lapply(sentiment_preds, corNoOut)
# Removing scientific notation for readability
options(scipen = 999)
# Coverts is_weekend into a two level factor
weekend_factor <- subset_data_wo_outliers %>%
mutate(weekend = as.factor(is_weekend)) %>% select(weekend, shares)
# Base plot
g <- ggplot(weekend_factor, aes(x=shares)) +  xlab("Number of Shares") +
ylab("Density")
# Filled, stacked histogram with density of shares by weekend level
g + geom_histogram(aes(y = ..density.., fill = weekend)) +
geom_density(adjust = 0.25, alpha = 0.5, aes(fill = weekend), position = "stack") +
labs(title = "Density of Shares: Weekday vs. Weekend") +
scale_fill_discrete(name = "Weekday or Weekend?", labels = c("Weekday", "Weekend"))
# Turning on scientific notation
options(scipen = 0)
# First linear model
lmfit1 <- lm(shares ~kw_avg_avg*average_token_length*n_tokens_content*n_tokens_title, data = training_data)
summary(lmfit1)
# Generalates second linear model candidate
lmfit2 <- lm(shares ~  n_tokens_content*num_imgs + n_tokens_content*num_videos +
n_tokens_content:average_token_length + n_tokens_content*kw_avg_avg +
n_tokens_content*global_sentiment_polarity +
n_tokens_content*global_rate_negative_words + num_imgs*kw_avg_avg +
num_imgs*abs_title_sentiment_polarity + num_videos*average_token_length +
num_videos*global_subjectivity + num_videos*global_sentiment_polarity +
num_videos*global_rate_negative_words + num_videos*abs_title_sentiment_polarity,
data = training_data)
summary(lmfit2)
fitTree <- tree(shares ~kw_avg_avg + average_token_length + n_tokens_content + n_tokens_title, data = training_data)
plot(fitTree)
text(fitTree)
#Random Forest
#Train control options for ensemble models
trCtrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
rfFit <- train(shares ~kw_avg_avg + average_token_length + n_tokens_content + n_tokens_title, data = training_data, method = "rf",trControl=trCtrl, preProcess = c("center", "scale"),tuneGrid = data.frame(mtry = 1:4))
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(rmarkdown)
# Creates a list of the six data_channel_is_* options
category <- c("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
# Creates output filenames
output_file <- paste0(category, ".html")
# Creates a list for each data channel with just the category parameter
params = lapply(category, FUN = function(x){list(category = x)})
# Stores list of file output file names and parameters as a data frame
reports <- tibble(output_file, params)
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project2.Rmd",
output_file = x[[1]],
params = x[[2]])})
set.seed(123)
#Required Packages
library(knitr)
library(caret)
library(tidyverse)
library(corrplot)
library(tree)
library(rmarkdown)
# Creates a list of the six data_channel_is_* options
category <- c("lifestyle", "entertainment", "bus", "socmed", "tech", "world")
# Creates output filenames
output_file <- paste0(category, ".md")
# Creates a list for each data channel with just the category parameter
params = lapply(category, FUN = function(x){list(category = x)})
# Stores list of file output file names and parameters as a data frame
reports <- tibble(output_file, params)
# Applies the render function to each pair of output file name and category
apply(reports, MARGIN = 1, FUN = function(x){ render(input = "Project2.Rmd",
output_file = x[[1]],
params = x[[2]])})
